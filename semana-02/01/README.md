# Clase 01 - Semana 02 - Ciclo de vida del Producto, calidad, fundamentos de DevOps y CI/CD

- Unidad 01: **Calidad y Testing de Software**
- Fecha: Lunes 18 de Agosto, 2025:
- Horario: 15:50 - 18:10
- Docente: Diego Obando

## ğŸ“ 1. MÃ©tricas de Calidad: Del concepto a la prÃ¡cticando

## ğŸ¯ Objetivos de la Clase

Al finalizar la clase seras capaz de:

- Integrar estrategias de testing en cada fase del ciclo de vida del desarrollo de software (SDLC).
- Implementar el concepto de "Shift-Left Testing" para mejorar la calidad del producto.
- Comprender las mÃ©tricas de calidad y quality gates en contextos DevOps.
- Aplicar los fundamentos de DevOps y CI/CD en procesos de testing automatizado.

---

## ğŸ“– Contenidos

### BLOQUE 1: Ciclo de vida del producto y Testing (35 min)

### BLOQUE 2: Calidad en contexto DevOps (35 min)

### BLOQUE 3: DevOps y CI/CD - Fundamentos (40 min)

### BLOQUE 4: Testing en pipelines CI/CD (30 min)

---

# ğŸ“¦ BLOQUE 1: Ciclo de vida del producto y Testing (35 min)

_"El testing no es una fase, es una mentalidad que debe estar presente en todo el ciclo de vida del producto"_

## ğŸ”„ 1. SDLC y Testing: MÃ¡s allÃ¡ de los conceptos bÃ¡sicos

Ya conocemos los niveles bÃ¡sicos de testing (unidad, integraciÃ³n, sistema) que vimos la semana pasada. Ahora vamos a **profundizar** en cÃ³mo el testing se integra estratÃ©gicamente en cada fase del desarrollo.

_SDLC acrÃ³nimo de Software Development Life Cycle_ es el proceso que abarca desde la concepciÃ³n de un producto hasta su despliegue y mantenimiento. En este bloque, veremos cÃ³mo el testing no es una fase aislada, sino una actividad que debe estar presente en cada etapa del SDLC.

### 1.1 El Modelo V Extendido ğŸ”€

```
DESARROLLO                    TESTING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Requisitos    â”‚ â—„â”€â”€â”€â”€â”€â”€â–º â”‚ Pruebas de      â”‚
â”‚                 â”‚          â”‚ AceptaciÃ³n      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   DiseÃ±o de     â”‚ â—„â”€â”€â”€â”€â”€â”€â–º â”‚ Pruebas de      â”‚
â”‚   Sistema       â”‚          â”‚ Sistema         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   DiseÃ±o        â”‚ â—„â”€â”€â”€â”€â”€â”€â–º â”‚ Pruebas de      â”‚
â”‚   Detallado     â”‚          â”‚ IntegraciÃ³n     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   CodificaciÃ³n  â”‚ â—„â”€â”€â”€â”€â”€â”€â–º â”‚ Pruebas         â”‚
â”‚                 â”‚          â”‚ Unitarias       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ” Punto clave:** El modelo V muestra cÃ³mo cada fase de desarrollo tiene su contraparte en testing, y ambas se planifican **simultÃ¡neamente**, no secuencialmente.

**ğŸ” Punto clave:** Cada fase de desarrollo tiene su contraparte en testing, y ambas se planifican **simultÃ¡neamente**, no secuencialmente.

### 1.2 Testing en cada fase del SDLC ğŸ“‹

| Fase SDLC                  | Actividades de Testing                                                                             | Entregables de Testing                                                                      | Valor Agregado                                               |
| -------------------------- | -------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| **AnÃ¡lisis de Requisitos** | â€¢ RevisiÃ³n de requisitos<br>â€¢ DefiniciÃ³n de criterios de aceptaciÃ³n<br>â€¢ IdentificaciÃ³n de riesgos | â€¢ Matriz de trazabilidad<br>â€¢ Casos de prueba de alto nivel                                 | DetecciÃ³n temprana de ambigÃ¼edades y requisitos no testeable |
| **DiseÃ±o**                 | â€¢ RevisiÃ³n de arquitectura<br>â€¢ DefiniciÃ³n de estrategia de testing<br>â€¢ DiseÃ±o de casos de prueba | â€¢ Plan de pruebas<br>â€¢ Casos de prueba detallados<br>â€¢ Datos de prueba                      | ValidaciÃ³n de que el diseÃ±o soporta los requisitos           |
| **ImplementaciÃ³n**         | â€¢ Testing unitario<br>â€¢ RevisiÃ³n de cÃ³digo<br>â€¢ AnÃ¡lisis estÃ¡tico                                  | â€¢ Suites de pruebas unitarias<br>â€¢ Reportes de cobertura<br>â€¢ Reportes de anÃ¡lisis estÃ¡tico | DetecciÃ³n de defectos a nivel de cÃ³digo                      |
| **IntegraciÃ³n**            | â€¢ Pruebas de integraciÃ³n<br>â€¢ Pruebas de API<br>â€¢ Testing de contratos                             | â€¢ Resultados de pruebas de integraciÃ³n<br>â€¢ DocumentaciÃ³n de interfaces                     | ValidaciÃ³n de interacciones entre componentes                |
| **Sistema**                | â€¢ Pruebas funcionales<br>â€¢ Pruebas no funcionales<br>â€¢ Testing de regresiÃ³n                        | â€¢ Resultados de pruebas de sistema<br>â€¢ MÃ©tricas de rendimiento<br>â€¢ Reportes de defectos   | ValidaciÃ³n del sistema completo                              |
| **Despliegue**             | â€¢ Pruebas de smoke<br>â€¢ Pruebas de aceptaciÃ³n<br>â€¢ Monitoreo post-despliegue                       | â€¢ Reportes de smoke testing<br>â€¢ Sign-off de aceptaciÃ³n<br>â€¢ MÃ©tricas de producciÃ³n         | ValidaciÃ³n en ambiente productivo                            |

## ğŸƒâ€â™‚ï¸ 2. Shift-Left Testing: La revoluciÃ³n del testing temprano

### 2.1 Â¿QuÃ© es Shift-Left Testing?

**DefiniciÃ³n:** Estrategia que mueve las actividades de testing hacia las **fases mÃ¡s tempranas** del ciclo de desarrollo.

```
TRADICIONAL:
Requisitos â†’ DiseÃ±o â†’ CÃ³digo â†’ Testing â†’ Despliegue
                              â†‘
                         Testing aquÃ­

SHIFT-LEFT:
Requisitos â†’ DiseÃ±o â†’ CÃ³digo â†’ Testing â†’ Despliegue
    â†‘         â†‘        â†‘
Testing desde aquÃ­ en todas las fases
```

### 2.2 Los 4 tipos de Shift-Left Testing ğŸ¯

#### Tipo 1: Shift-Left Traditional ğŸ“š

- **QuÃ© es:** Comenzar el diseÃ±o de pruebas durante las fases de requisitos y diseÃ±o
- **CuÃ¡ndo:** Proyectos con metodologÃ­as tradicionales (Waterfall, V-Model)
- **Ejemplo prÃ¡ctico:**
  ```
  Requisito: "El usuario debe poder cambiar su contraseÃ±a"
  Testing temprano:
  âœ… Â¿QuÃ© validaciones debe tener la nueva contraseÃ±a?
  âœ… Â¿CÃ³mo se notifica al usuario del cambio exitoso?
  âœ… Â¿QuÃ© pasa si falla el cambio?
  ```

#### Tipo 2: Shift-Left Incremental ğŸ”„

- **QuÃ© es:** Testing continuo en cada iteraciÃ³n/sprint
- **CuÃ¡ndo:** MetodologÃ­as Ã¡giles (Scrum, Kanban)
- **Ejemplo prÃ¡ctico:**
  ```
  Sprint Planning:
  - Historia de usuario definida
  - Criterios de aceptaciÃ³n claros
  - Casos de prueba diseÃ±ados
  - Definition of Done incluye testing
  ```

#### Tipo 3: Shift-Left Agile/DevOps ğŸš€

- **QuÃ© es:** Developers escriben y ejecutan sus propias pruebas
- **CuÃ¡ndo:** Equipos DevOps maduros
- **Ejemplo prÃ¡ctico:**
  ```javascript
  // Developer escribe test ANTES del cÃ³digo
  test("changePassword should validate minimum length", () => {
    expect(() => changePassword("123")).toThrow("Password too short");
  });
  ```

#### Tipo 4: Shift-Left Model-Based ğŸ§ 

- **QuÃ© es:** Uso de modelos para generar automÃ¡ticamente casos de prueba
- **CuÃ¡ndo:** Sistemas complejos con modelos formales
- **Ejemplo:** Diagramas de estado que generan automÃ¡ticamente tests

### 2.3 Beneficios del Shift-Left Testing ğŸ“ˆ

| Beneficio                | Sin Shift-Left            | Con Shift-Left              |
| ------------------------ | ------------------------- | --------------------------- |
| **Costo de defectos**    | Alto (encontrados tarde)  | Bajo (encontrados temprano) |
| **Tiempo de feedback**   | DÃ­as/semanas              | Minutos/horas               |
| **Calidad del cÃ³digo**   | Reactiva                  | Proactiva                   |
| **Cobertura de testing** | Limitada por tiempo       | Integral desde el inicio    |
| **ColaboraciÃ³n equipo**  | Testing como "gatekeeper" | Testing como enabler        |

### 2.4 ImplementaciÃ³n prÃ¡ctica de Shift-Left ğŸ› ï¸

#### Checklist para implementar Shift-Left:

**En la fase de Requisitos:**

- [ ] Cada requisito tiene criterios de aceptaciÃ³n claros y testeables
- [ ] Se han identificado casos edge y escenarios negativos
- [ ] Existe trazabilidad entre requisitos y casos de prueba

**En la fase de DiseÃ±o:**

- [ ] La arquitectura considera la testabilidad (dependency injection, etc.)
- [ ] Se han definido interfaces claras para facilitar mocking
- [ ] Existe una estrategia de datos de prueba

**En la fase de Desarrollo:**

- [ ] Developers escriben unit tests antes o durante el desarrollo
- [ ] Code reviews incluyen revisiÃ³n de tests
- [ ] CI/CD ejecuta automÃ¡ticamente las pruebas en cada commit

## ğŸ”„ 3. Testing Continuo vs Testing Tradicional

### 3.1 Comparativa detallada

| Aspecto             | Testing Tradicional            | Testing Continuo                    |
| ------------------- | ------------------------------ | ----------------------------------- |
| **Momento**         | Fase dedicada post-desarrollo  | Integrado en todo el ciclo          |
| **Frecuencia**      | Lotes grandes, menos frecuente | PequeÃ±os incrementos, muy frecuente |
| **Feedback**        | Al final del ciclo             | Inmediato                           |
| **AutomatizaciÃ³n**  | Limitada                       | Extensiva                           |
| **Responsabilidad** | Equipo de QA separado          | Todo el equipo                      |
| **MÃ©tricas**        | Defectos encontrados           | Defectos prevenidos                 |

### 3.2 El flujo del Testing Continuo ğŸŒŠ

```
CÃ³digo â†’ Build â†’ Unit Tests â†’ Integration Tests â†’ Deploy to Test â†’
System Tests â†’ Performance Tests â†’ Security Tests â†’ Deploy to Prod â†’
Monitor â†’ Feedback â†’ CÃ³digo (nuevo ciclo)
```

**ğŸ¯ Punto clave:** En testing continuo, cada cambio de cÃ³digo activa automÃ¡ticamente una serie de validaciones que proporcionan feedback inmediato.

## ğŸ’¡ Ejercicio PrÃ¡ctico (10 min)

**Escenario:** Tu equipo estÃ¡ desarrollando una aplicaciÃ³n de e-commerce.

**Requisito nuevo:** "Implementar sistema de descuentos por cÃ³digo promocional"

**Tarea en grupos (3-4 personas):**

1. Identifica en quÃ© fases del SDLC aplicarÃ­as testing
2. Define 3 actividades de Shift-Left Testing para este requisito
3. DiseÃ±a 2 casos de prueba que escribirÃ­as ANTES de programar

**Tiempo:** 7 minutos de discusiÃ³n + 3 minutos de puesta en comÃºn

---

## ğŸ”š Resumen BLOQUE 1

**Conceptos clave cubiertos:**
âœ… SDLC extendido con testing integrado
âœ… 4 tipos de Shift-Left Testing
âœ… Testing continuo vs tradicional
âœ… ImplementaciÃ³n prÃ¡ctica

**Para el siguiente bloque:**
ğŸ¯ MÃ©tricas de calidad y quality gates en DevOps

---

# ğŸ“Š BLOQUE 2: Calidad en contexto DevOps (35 min)

_"No puedes mejorar lo que no puedes medir, pero las mÃ©tricas sin acciÃ³n son solo nÃºmeros bonitos"_

## ğŸ“š Glosario de TÃ©rminos Clave

Antes de profundizar, definamos los conceptos y acrÃ³nimos que usaremos:

| TÃ©rmino/AcrÃ³nimo          | DefiniciÃ³n                                                                                                                        | Ejemplo                                                            |
| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| **DevOps**                | Development + Operations. Cultura y prÃ¡cticas que combinan desarrollo y operaciones para entregar software mÃ¡s rÃ¡pido y confiable | Equipos que despliegan cÃ³digo varias veces al dÃ­a                  |
| **CI/CD**                 | Continuous Integration / Continuous Deployment. AutomatizaciÃ³n del proceso de integraciÃ³n y despliegue de cÃ³digo                  | GitHub Actions que ejecuta tests automÃ¡ticamente en cada push      |
| **DORA**                  | DevOps Research and Assessment. OrganizaciÃ³n que define las 4 mÃ©tricas clave de rendimiento DevOps                                | Las mÃ©tricas que miden quÃ© tan bien funciona tu equipo             |
| **MTTR**                  | Mean Time To Recovery. Tiempo promedio para recuperarse de un fallo en producciÃ³n                                                 | Si el sistema falla, Â¿cuÃ¡nto tardamos en arreglarlo?               |
| **SLA/SLO**               | Service Level Agreement/Objective. Acuerdos sobre nivel de servicio esperado                                                      | "El sistema debe estar disponible 99.9% del tiempo"                |
| **Lead Time**             | Tiempo desde que se hace un commit hasta que llega a producciÃ³n                                                                   | Desde que escribes cÃ³digo hasta que los usuarios lo usan           |
| **Technical Debt**        | Costo de refactorizaciÃ³n necesaria por tomar atajos en el desarrollo                                                              | CÃ³digo mal escrito que funciona pero es difÃ­cil de mantener        |
| **Quality Gate**          | Punto de control automatizado que bloquea cÃ³digo que no cumple criterios de calidad                                               | "No pasa a producciÃ³n si tiene menos de 80% de cobertura de tests" |
| **Mutation Testing**      | TÃ©cnica que introduce errores artificiales en el cÃ³digo para probar la calidad de los tests                                       | Si cambio un ">" por "<", Â¿mis tests detectan el error?            |
| **Code Coverage**         | Porcentaje del cÃ³digo que es ejecutado por las pruebas automatizadas                                                              | "Mis tests ejecutan el 85% de las lÃ­neas de cÃ³digo"                |
| **Cyclomatic Complexity** | MÃ©trica que mide la complejidad del cÃ³digo basado en el nÃºmero de caminos independientes                                          | Una funciÃ³n con muchos `if` y `loops` tiene alta complejidad       |
| **Feedback Loop**         | Tiempo que tarda en llegar informaciÃ³n sobre si algo funciona bien o mal                                                          | Desde que haces un cambio hasta que sabes si funcionÃ³              |

## ğŸ“ 1. MÃ©tricas de Calidad: Del concepto a la prÃ¡ctica

Ya sabemos que la calidad es importante. Ahora necesitamos **medirla**, **monitorearla** y **actuar** basados en datos concretos.

### 1.1 PirÃ¡mide de MÃ©tricas de Calidad ğŸ—ï¸

```mermaid
graph TD
    A[ğŸ“Š MÃ©tricas de Negocio] --> B[ğŸ“ˆ MÃ©tricas de Producto]
    B --> C[âš™ï¸ MÃ©tricas de Proceso]
    C --> D[ğŸ”§ MÃ©tricas TÃ©cnicas]

    A1[User Satisfaction<br/>Revenue Impact<br/>Churn Rate]
    B1[Feature Adoption<br/>Performance<br/>Availability]
    C1[Lead Time<br/>Deploy Frequency<br/>MTTR]
    D1[Code Coverage<br/>Cyclomatic Complexity<br/>Technical Debt]

    A --> A1
    B --> B1
    C --> C1
    D --> D1
```

### 1.2 MÃ©tricas TÃ©cnicas de Calidad ğŸ”§

#### MÃ©tricas de Cobertura

> ğŸ’¡ **Â¿QuÃ© es Code Coverage?** Es el porcentaje de cÃ³digo que es ejecutado cuando corren nuestras pruebas. Si tengo 100 lÃ­neas de cÃ³digo y mis tests ejecutan 80 lÃ­neas, tengo 80% de cobertura.

| MÃ©trica               | DescripciÃ³n                       | Valor Target | âš ï¸ Cuidado                       |
| --------------------- | --------------------------------- | ------------ | -------------------------------- |
| **Line Coverage**     | % lÃ­neas ejecutadas por tests     | 70-80%       | Alta cobertura â‰  buenos tests    |
| **Branch Coverage**   | % ramas de condiciones probadas   | 80-90%       | MÃ¡s importante que line coverage |
| **Function Coverage** | % funciones invocadas             | 90-95%       | Identifica cÃ³digo no utilizado   |
| **Mutation Coverage** | % mutaciones detectadas por tests | 60-70%       | Mide calidad real de los tests   |

**ğŸ“ Explicaciones:**

- **Line Coverage:** Si mi funciÃ³n tiene 10 lÃ­neas y los tests ejecutan 8, tengo 80% line coverage
- **Branch Coverage:** Si tengo un `if/else` y solo pruebo el `if`, tengo 50% branch coverage
- **Function Coverage:** Si tengo 5 funciones y solo llamo a 4 en mis tests, tengo 80% function coverage
- **Mutation Coverage:** Cambia automÃ¡ticamente `>` por `<` en el cÃ³digo. Si mis tests siguen pasando, estÃ¡n mal hechos

#### MÃ©tricas de Complejidad

> ğŸ’¡ **Â¿QuÃ© es Complejidad de CÃ³digo?** Es quÃ© tan difÃ­cil es entender, modificar y mantener el cÃ³digo. CÃ³digo complejo = mÃ¡s bugs y mÃ¡s tiempo de desarrollo.

| MÃ©trica                   | DescripciÃ³n                            | Valor Target              | AcciÃ³n si excede          |
| ------------------------- | -------------------------------------- | ------------------------- | ------------------------- |
| **Cyclomatic Complexity** | NÃºmero de caminos independientes       | < 10 por funciÃ³n          | Refactorizar funciÃ³n      |
| **Cognitive Complexity**  | Dificultad mental para entender cÃ³digo | < 15 por funciÃ³n          | Simplificar lÃ³gica        |
| **Depth of Inheritance**  | Niveles de herencia                    | < 5 niveles               | Revisar diseÃ±o OO         |
| **Lines of Code (LOC)**   | LÃ­neas por funciÃ³n/clase               | < 50 funciÃ³n, < 500 clase | Dividir responsabilidades |

**ğŸ“ Explicaciones:**

- **Cyclomatic Complexity:** Cuenta `if`, `while`, `for`, `case`. Una funciÃ³n con 3 `if` tiene complexity â‰ˆ 4
- **Cognitive Complexity:** Mide esfuerzo mental. Nested loops y condiciones suman mÃ¡s puntos
- **Depth of Inheritance:** Clase A â†’ Clase B â†’ Clase C = profundidad 3
- **LOC:** Simplemente cuenta lÃ­neas. Funciones largas son difÃ­ciles de entender

### 1.3 DORA Metrics: Las 4 mÃ©tricas clave de DevOps ğŸ¯

```mermaid
graph LR
    subgraph "ğŸ“Š DORA Metrics"
        A[âš¡ Deployment<br/>Frequency]
        B[ğŸ“ˆ Lead Time<br/>for Changes]
        C[ğŸ› ï¸ Mean Time<br/>to Recovery]
        D[âŒ Change Failure<br/>Rate]
    end

    subgraph "ğŸ† Elite Performance"
        A1[Multiple deploys<br/>per day]
        B1[< 1 hour]
        C1[< 1 hour]
        D1[0-15%]
    end

    A --> A1
    B --> B1
    C --> C1
    D --> D1
```

#### Detalle de DORA Metrics:

**1. Deployment Frequency (Frecuencia de Despliegue)**

- **QuÃ© mide:** Â¿Con quÃ© frecuencia desplegamos cÃ³digo a producciÃ³n?
- **Por quÃ© importa:** Indica la capacidad del equipo para entregar valor
- **Niveles de performance:**
  - ğŸ† Elite: Multiple deploys per day
  - ğŸ¥‡ High: Between once per day and once per week
  - ğŸ¥ˆ Medium: Between once per week and once per month
  - ğŸ¥‰ Low: Between once per month and once every 6 months

**2. Lead Time for Changes (Tiempo desde commit hasta producciÃ³n)**

- **QuÃ© mide:** Â¿CuÃ¡nto tiempo tarda un cambio en llegar a producciÃ³n?
- **Por quÃ© importa:** Mide la eficiencia del pipeline de desarrollo
- **CÃ³mo mejorar:** AutomatizaciÃ³n, CI/CD, reducir batch size

**3. Mean Time to Recovery (MTTR)**

- **QuÃ© mide:** Â¿CuÃ¡nto tiempo tomamos en recuperarnos de un fallo?
- **Por quÃ© importa:** Indica la resilencia y capacidad de respuesta
- **CÃ³mo mejorar:** Monitoring, alertas, rollback automÃ¡tico

**4. Change Failure Rate**

- **QuÃ© mide:** Â¿QuÃ© % de deploys resultan en fallo en producciÃ³n?
- **Por quÃ© importa:** Indica la calidad de nuestros procesos de testing
- **Ejemplo prÃ¡ctico:** De 10 veces que subimos cÃ³digo, Â¿cuÃ¡ntas veces algo se rompe?
- **CÃ³mo mejorar:** Mejor testing, feature flags, canary deployments

> ğŸ’¡ **Â¿QuÃ© son Feature Flags?** Son interruptores en el cÃ³digo que permiten activar/desactivar funcionalidades sin redesplegar. Como un switch de luz para funciones del software.

> ğŸ’¡ **Â¿QuÃ© es Canary Deployment?** Es desplegar la nueva versiÃ³n solo para un pequeÃ±o porcentaje de usuarios primero. Si funciona bien, se despliega para todos.

## ğŸšª 2. Quality Gates: Los guardianes de la calidad

### 2.1 Â¿QuÃ© son los Quality Gates?

**DefiniciÃ³n:** Puntos de control automatizados que **bloquean** el avance del cÃ³digo si no cumple criterios de calidad predefinidos.

> ğŸ’¡ **AnalogÃ­a:** Es como un control de seguridad en el aeropuerto. Si no cumples los requisitos, no pasas. En software, si el cÃ³digo no cumple estÃ¡ndares de calidad, no avanza al siguiente ambiente.

```mermaid
graph LR
    A[Developer<br/>Commit] --> B{Quality Gate 1<br/>Unit Tests}
    B -->|âœ… Pass| C{Quality Gate 2<br/>Code Quality}
    B -->|âŒ Fail| A
    C -->|âœ… Pass| D{Quality Gate 3<br/>Integration Tests}
    C -->|âŒ Fail| A
    D -->|âœ… Pass| E{Quality Gate 4<br/>Security Scan}
    D -->|âŒ Fail| A
    E -->|âœ… Pass| F[Deploy to<br/>Staging]
    E -->|âŒ Fail| A
```

### 2.2 ConfiguraciÃ³n de Quality Gates por ambiente ğŸšï¸

#### Quality Gate - Development

```yaml
# Ejemplo configuraciÃ³n SonarQube
sonar.qualitygate.wait=true
conditions:
  - Coverage > 80%
  - Duplicated Lines < 3%
  - Maintainability Rating = A
  - Reliability Rating = A
  - Security Rating = A
  - New Code Coverage > 85%
```

#### Quality Gate - Production

```yaml
# MÃ¡s estricto para producciÃ³n
conditions:
  - Coverage > 90%
  - Duplicated Lines < 1%
  - Critical Vulnerabilities = 0
  - High Vulnerabilities < 5
  - Performance Tests Pass = 100%
  - Load Tests Pass = 100%
```

### 2.3 ImplementaciÃ³n prÃ¡ctica de Quality Gates ğŸ› ï¸

> ğŸ’¡ **Herramientas explicadas:**
>
> - **SonarQube:** Analiza cÃ³digo para encontrar bugs, vulnerabilidades y "code smells" (cÃ³digo que funciona pero estÃ¡ mal escrito)
> - **JaCoCo/Jest:** Herramientas que miden quÃ© porcentaje del cÃ³digo estÃ¡ cubierto por tests
> - **OWASP ZAP:** Herramienta que busca vulnerabilidades de seguridad en aplicaciones web
> - **JMeter/k6:** Herramientas que simulan muchos usuarios usando la aplicaciÃ³n para medir rendimiento

| Etapa Pipeline   | Quality Gate              | Herramienta  | AcciÃ³n si falla         |
| ---------------- | ------------------------- | ------------ | ----------------------- |
| **Build**        | CompilaciÃ³n exitosa       | Maven/Gradle | Notificar desarrollador |
| **Unit Tests**   | Coverage > 80%            | JaCoCo/Jest  | Bloquear merge          |
| **Code Quality** | No code smells crÃ­ticos   | SonarQube    | Crear issue automÃ¡tico  |
| **Security**     | No vulnerabilidades altas | OWASP ZAP    | Bloquear despliegue     |
| **Performance**  | Response time < 200ms     | JMeter/k6    | Rollback automÃ¡tico     |

## ğŸ”„ 3. Feedback Loops: El corazÃ³n de la mejora continua

> ğŸ’¡ **Â¿QuÃ© es un Feedback Loop?** Es el tiempo que tarda en llegar informaciÃ³n sobre si algo funciona bien o mal. Entre mÃ¡s rÃ¡pido el feedback, mÃ¡s rÃ¡pido puedes corregir errores.

### 3.1 Tipos de Feedback Loops

**ğŸ“ Explicaciones de tipos de feedback:**

- **IDE Linting:** Tu editor de cÃ³digo te dice inmediatamente si hay errores de sintaxis (lÃ­neas rojas)
- **Static Analysis:** Analiza el cÃ³digo sin ejecutarlo para encontrar posibles problemas
- **E2E Tests:** End-to-End, pruebas que simulan un usuario real usando toda la aplicaciÃ³n
- **A/B Testing:** Mostrar dos versiones diferentes a usuarios para ver cuÃ¡l funciona mejor

### 3.1 Tipos de Feedback Loops

```mermaid
graph TD
    subgraph "âš¡ Fast Feedback (< 10 min)"
        A[IDE Linting]
        B[Unit Tests]
        C[Static Analysis]
    end

    subgraph "ğŸš€ Medium Feedback (10-60 min)"
        D[Integration Tests]
        E[Code Quality Gates]
        F[Security Scanning]
    end

    subgraph "ğŸ¯ Slow Feedback (1-24 hours)"
        G[E2E Tests]
        H[Performance Tests]
        I[User Acceptance]
    end

    subgraph "ğŸ“Š Business Feedback (days-weeks)"
        J[User Metrics]
        K[Business KPIs]
        L[A/B Test Results]
    end
```

### 3.2 Principios de Feedback efectivo ğŸ“¡

#### The 3 R's of Feedback:

1. **Rapid (RÃ¡pido):** Feedback inmediato cuando es posible
2. **Relevant (Relevante):** InformaciÃ³n especÃ­fica y accionable
3. **Reliable (Confiable):** Consistente y preciso

#### ImplementaciÃ³n prÃ¡ctica:

```javascript
// Ejemplo: Feedback inmediato en IDE
// ESLint + Prettier + TypeScript
{
  "scripts": {
    "lint:fix": "eslint --fix src/",
    "test:watch": "jest --watch --coverage",
    "type:check": "tsc --noEmit"
  },
  "husky": {
    "hooks": {
      "pre-commit": "lint-staged && npm test"
    }
  }
}
```

## ğŸ“Š 4. Dashboards y VisualizaciÃ³n de MÃ©tricas

### 4.1 Dashboard de Calidad - Ejemplo estructura

```mermaid
graph TB
    subgraph "ğŸ¯ Executive Dashboard"
        A1[DORA Metrics]
        A2[Business KPIs]
        A3[Quality Trends]
    end

    subgraph "ğŸ‘¨â€ğŸ’» Team Dashboard"
        B1[Test Coverage]
        B2[Build Success Rate]
        B3[Code Quality]
        B4[Deployment Status]
    end

    subgraph "ğŸ”§ Technical Dashboard"
        C1[Performance Metrics]
        C2[Error Rates]
        C3[Infrastructure Health]
        C4[Security Vulnerabilities]
    end
```

#### ğŸ“‹ ExplicaciÃ³n detallada de cada Dashboard:

### ğŸ¯ Executive Dashboard (Para directivos y gerentes)

**PropÃ³sito:** Vista de alto nivel para tomar decisiones estratÃ©gicas

| MÃ©trica            | Â¿QuÃ© muestra?                                | Ejemplo prÃ¡ctico                                       | Â¿Por quÃ© importa?                                 |
| ------------------ | -------------------------------------------- | ------------------------------------------------------ | ------------------------------------------------- |
| **DORA Metrics**   | Las 4 mÃ©tricas clave que ya vimos            | "Desplegamos 3 veces por semana, Lead Time de 2 horas" | Mide la eficiencia del equipo de desarrollo       |
| **Business KPIs**  | MÃ©tricas de negocio relacionadas con calidad | "Customer satisfaction: 4.2/5, Churn rate: 5%"         | Conecta calidad tÃ©cnica con resultados de negocio |
| **Quality Trends** | EvoluciÃ³n de la calidad en el tiempo         | "Los bugs en producciÃ³n bajaron 40% este trimestre"    | Muestra si las inversiones en calidad funcionan   |

### ğŸ‘¨â€ğŸ’» Team Dashboard (Para desarrolladores y QA)

**PropÃ³sito:** MÃ©tricas operativas diarias para el equipo de desarrollo

| MÃ©trica                | Â¿QuÃ© muestra?                             | Ejemplo prÃ¡ctico                                  | Â¿CÃ³mo actuar?                                  |
| ---------------------- | ----------------------------------------- | ------------------------------------------------- | ---------------------------------------------- |
| **Test Coverage**      | % del cÃ³digo cubierto por pruebas         | "Coverage actual: 78%, objetivo: 80%"             | Si baja del objetivo, agregar mÃ¡s tests        |
| **Build Success Rate** | % de builds que pasan sin errores         | "95% de builds exitosos esta semana"              | Si baja mucho, revisar procesos de development |
| **Code Quality**       | MÃ©tricas de SonarQube (bugs, code smells) | "3 bugs crÃ­ticos, 12 code smells menores"         | Priorizar correcciÃ³n de issues crÃ­ticos        |
| **Deployment Status**  | Estado actual de deployments              | "Prod: âœ… Stable, Staging: âš ï¸ Deploy in progress" | Visibilidad del estado de ambientes            |

### ğŸ”§ Technical Dashboard (Para DevOps y SRE)

**PropÃ³sito:** Monitoreo tÃ©cnico de sistemas en tiempo real

> ğŸ’¡ **Â¿QuÃ© es SRE?** Site Reliability Engineering - Equipos que se encargan de que los sistemas funcionen de manera confiable en producciÃ³n.

| MÃ©trica                      | Â¿QuÃ© muestra?                              | Ejemplo prÃ¡ctico                             | Â¿CuÃ¡ndo alertar?                         |
| ---------------------------- | ------------------------------------------ | -------------------------------------------- | ---------------------------------------- |
| **Performance Metrics**      | Tiempo de respuesta, throughput            | "API response time: 150ms avg, 1200 req/min" | Si response time > 500ms                 |
| **Error Rates**              | % de requests que fallan                   | "Error rate: 0.1% (5 errors/5000 requests)"  | Si error rate > 1%                       |
| **Infrastructure Health**    | CPU, memoria, disco de servidores          | "CPU: 45%, Memory: 67%, Disk: 23%"           | Si CPU > 80% por 5+ minutos              |
| **Security Vulnerabilities** | Vulnerabilidades detectadas en tiempo real | "2 medium vulnerabilities detected"          | Vulnerabilidades crÃ­ticas inmediatamente |

#### ğŸ¨ Principios de diseÃ±o para Dashboards efectivos:

1. **JerarquÃ­a visual:** Lo mÃ¡s importante debe ser mÃ¡s visible
2. **CÃ³digos de color consistentes:** Verde = bien, Amarillo = atenciÃ³n, Rojo = problema
3. **InformaciÃ³n accionable:** Cada mÃ©trica debe sugerir quÃ© hacer
4. **ActualizaciÃ³n en tiempo real:** Los datos deben ser frescos y confiables

#### ğŸ’» Ejemplo de implementaciÃ³n prÃ¡ctica:

```yaml
# ConfiguraciÃ³n ejemplo para Grafana Dashboard
dashboard:
  - title: "Team Dashboard"
    panels:
      - title: "Test Coverage"
        type: "stat"
        targets:
          - expr: "sonarqube_coverage_percentage"
        thresholds:
          - color: "red"
            value: 0
          - color: "yellow"
            value: 70
          - color: "green"
            value: 80

      - title: "Build Success Rate"
        type: "stat"
        targets:
          - expr: "rate(jenkins_builds_success_total[7d])"
        unit: "percent"
```

### 4.2 Herramientas recomendadas ğŸ› ï¸

| CategorÃ­a        | Herramientas                       | PropÃ³sito                        |
| ---------------- | ---------------------------------- | -------------------------------- |
| **Code Quality** | SonarQube, CodeClimate             | AnÃ¡lisis estÃ¡tico, deuda tÃ©cnica |
| **Testing**      | Jest, JUnit, Cypress               | EjecuciÃ³n y reporte de pruebas   |
| **Performance**  | Grafana, New Relic                 | Monitoreo de performance         |
| **CI/CD**        | Jenkins, GitLab CI, GitHub Actions | Pipeline automation              |
| **Dashboards**   | Grafana, DataDog, Elastic          | VisualizaciÃ³n de mÃ©tricas        |

## ğŸ’¡ Ejercicio PrÃ¡ctico (10 min)

**Escenario:** Tu equipo tiene estos nÃºmeros actuales:

- Deployment Frequency: 1 vez por semana
- Lead Time: 3 dÃ­as
- MTTR: 4 horas
- Change Failure Rate: 25%
- Test Coverage: 65%

**Tarea individual (5 min):**

1. Identifica cuÃ¡l es tu mÃ©trica mÃ¡s crÃ­tica
2. Define 2 acciones concretas para mejorarla
3. PropÃ³n 1 Quality Gate que ayudarÃ­a

**Puesta en comÃºn (5 min):** DiscusiÃ³n grupal de soluciones

---

## ğŸ”š Resumen BLOQUE 2

**Conceptos clave cubiertos:**
âœ… PirÃ¡mide de mÃ©tricas de calidad
âœ… DORA Metrics y su importancia  
âœ… Quality Gates automatizados
âœ… Feedback loops efectivos
âœ… Dashboards y visualizaciÃ³n

**Para el siguiente bloque:**
ğŸ¯ Fundamentos de DevOps y cultura CAMS

---

# ğŸš€ BLOQUE 3: DevOps y CI/CD - Fundamentos (40 min)

_"DevOps no es una herramienta, no es un rol, es una cultura que transforma cÃ³mo desarrollamos y entregamos software"_

## ğŸ“š Glosario adicional para DevOps

| TÃ©rmino/AcrÃ³nimo           | DefiniciÃ³n                                                            | Ejemplo prÃ¡ctico                                          |
| -------------------------- | --------------------------------------------------------------------- | --------------------------------------------------------- |
| **DevOps**                 | Cultura que unifica Development (Dev) y Operations (Ops)              | Los desarrolladores pueden desplegar su propio cÃ³digo     |
| **CI/CD**                  | Continuous Integration / Continuous Deployment                        | Cada push al repositorio activa build y deploy automÃ¡tico |
| **CAMS**                   | Culture, Automation, Measurement, Sharing - pilares de DevOps         | Las 4 Ã¡reas clave para implementar DevOps exitosamente    |
| **Infrastructure as Code** | Gestionar infraestructura usando cÃ³digo versionado                    | Crear servidores usando archivos de configuraciÃ³n         |
| **Microservicios**         | Arquitectura de aplicaciones como servicios pequeÃ±os e independientes | En lugar de 1 app monolÃ­tica, tener 10 servicios pequeÃ±os |
| **Containers**             | TecnologÃ­a para empaquetar aplicaciones con sus dependencias          | Docker containers que corren igual en dev y producciÃ³n    |
| **Pipeline**               | Secuencia automatizada de pasos para procesar cÃ³digo                  | Code â†’ Build â†’ Test â†’ Deploy                              |

## ğŸ¯ 1. Â¿QuÃ© es DevOps realmente?

### 1.1 EvoluciÃ³n histÃ³rica: Del caos a la colaboraciÃ³n

```mermaid
graph LR
    subgraph "ğŸšï¸ Antes (Silos)"
        A1[Developers<br/>ğŸ§‘â€ğŸ’»]
        A2[QA<br/>ğŸ§ª]
        A3[Operations<br/>âš™ï¸]
    end

    subgraph "ğŸ¤ DevOps (Colaborativo)"
        B1[Dev+Ops Team<br/>ğŸš€<br/>Cross-functional]
    end

    A1 -.->|"Throw code<br/>over the wall"| A2
    A2 -.->|"Throw bugs<br/>back"| A1
    A2 -.->|"Deploy when<br/>ready"| A3
    A3 -.->|"Production<br/>issues"| A1

    A1 --> B1
    A2 --> B1
    A3 --> B1
```

**ğŸ“Š Problemas del modelo tradicional:**

- **Silos organizacionales:** Cada equipo trabaja aislado
- **Handoffs lentos:** "Tiramos el cÃ³digo por encima del muro"
- **Blame culture:** "No es mi problema, es del otro equipo"
- **Deploy traumÃ¡tico:** Releases cada 6 meses, llenos de bugs
- **Feedback tardÃ­o:** Problemas se descubren en producciÃ³n

**âœ… Beneficios del modelo DevOps:**

- **ColaboraciÃ³n continua:** Todos trabajan hacia el mismo objetivo
- **Ownership compartido:** "You build it, you run it"
- **Feedback rÃ¡pido:** Problemas detectados en minutos, no meses
- **Deploys frecuentes:** MÃºltiples releases por dÃ­a
- **Mayor calidad:** Menos bugs, mÃ¡s estabilidad

### 1.2 DevOps no es solo herramientas

> âš ï¸ **Error comÃºn:** "Implementamos Jenkins y Docker, ya tenemos DevOps"

**âŒ DevOps NO ES:**

- Una herramienta especÃ­fica
- Un tÃ­tulo de trabajo ("DevOps Engineer")
- Solo automatizaciÃ³n
- Reemplazar QA con developers
- Solo para empresas tech grandes

**âœ… DevOps SÃ ES:**

- Una **cultura** de colaboraciÃ³n
- Un conjunto de **prÃ¡cticas** para entregar valor rÃ¡pido
- Una **mentalidad** de mejora continua
- **Responsabilidad compartida** de la calidad
- **AutomatizaciÃ³n** como medio, no como fin

## ğŸ—ï¸ 2. Los 4 pilares de DevOps: Modelo CAMS

### 2.1 ğŸ“š Culture (Cultura) - El pilar mÃ¡s importante

**Â¿QuÃ© significa?**
Cambiar la mentalidad de "nosotros vs ellos" a "todos somos un equipo"

**CaracterÃ­sticas de una cultura DevOps:**
| Antes | DespuÃ©s |
|-------|---------|
| "No es mi cÃ³digo, no es mi problema" | "Todos somos responsables del producto final" |
| "Funciona en mi mÃ¡quina" | "Funciona en producciÃ³n o no funciona" |
| "QA encontrarÃ¡ los bugs" | "Calidad es responsabilidad de todos" |
| "Deploys los viernes estÃ¡n bien" | "Deploys pequeÃ±os y frecuentes" |

**ğŸ› ï¸ CÃ³mo implementar cultura DevOps:**

- **Blameless postmortems:** Cuando algo falla, enfocarse en el proceso, no en culpar personas
- **Cross-functional teams:** Equipos con devs, QA, ops, product managers
- **Shared metrics:** Todas las teams medidas con las mismas mÃ©tricas (DORA)
- **Learning culture:** Experimentar, fallar rÃ¡pido, aprender, iterar

### 2.2 ğŸ¤– Automation (AutomatizaciÃ³n) - Eliminar el trabajo manual

**Â¿Por quÃ© automatizar?**

- Los humanos cometen errores en tareas repetitivas
- La automatizaciÃ³n es consistente y repetible
- Libera tiempo para trabajo de mayor valor

**ğŸ”§ Ãreas clave de automatizaciÃ³n:**

```mermaid
graph TD
    A[ğŸ¤– AutomatizaciÃ³n DevOps] --> B[Code Integration]
    A --> C[Testing]
    A --> D[Deployment]
    A --> E[Infrastructure]
    A --> F[Monitoring]

    B --> B1[Git hooks<br/>Code reviews<br/>Branch policies]
    C --> C1[Unit tests<br/>Integration tests<br/>Security scans]
    D --> D1[Blue-green deploys<br/>Rollbacks<br/>Feature flags]
    E --> E1[Infrastructure as Code<br/>Auto-scaling<br/>Provisioning]
    F --> F1[Alerts<br/>Log aggregation<br/>Health checks]
```

**ğŸ’» Ejemplo de automatizaciÃ³n prÃ¡ctica:**

```yaml
# Pipeline CI/CD automatizado
trigger:
  - main

stages:
  - stage: Build
    jobs:
      - job: CompileAndTest
        steps:
          - script: npm install
          - script: npm run build
          - script: npm test
          - script: npm run coverage

  - stage: QualityGates
    dependsOn: Build
    jobs:
      - job: SecurityScan
        steps:
          - script: npm audit
          - script: sonarqube-scanner

  - stage: Deploy
    dependsOn: QualityGates
    condition: succeeded()
    jobs:
      - job: DeployToProduction
        steps:
          - script: docker build -t myapp .
          - script: kubectl apply -f k8s/
```

### 2.3 ğŸ“Š Measurement (MediciÃ³n) - Lo que no se mide, no se mejora

**Â¿QuÃ© medimos en DevOps?**

**ğŸ“ˆ MÃ©tricas de flujo (ya vimos DORA):**

- Deployment Frequency
- Lead Time for Changes
- Mean Time to Recovery
- Change Failure Rate

**ğŸ“Š MÃ©tricas de calidad:**

- Automated test coverage
- Defect density
- Customer satisfaction
- System availability

**ğŸ¯ MÃ©tricas de equipo:**

- Team velocity
- Cycle time
- Work in progress (WIP)
- Employee engagement

**ğŸ“‹ ImplementaciÃ³n prÃ¡ctica de mediciÃ³n:**

```javascript
// Ejemplo: MÃ©tricas automÃ¡ticas en cÃ³digo
const metrics = {
  deployments: {
    frequency: trackDeployments(),
    leadTime: calculateLeadTime(),
    failureRate: calculateFailureRate(),
  },
  quality: {
    coverage: getCoverageFromJest(),
    bugs: getFromJira("type = Bug"),
    performance: getResponseTimes(),
  },
};

// Enviar mÃ©tricas a dashboard
sendMetrics(metrics);
```

### 2.4 ğŸ¤ Sharing (Compartir) - El conocimiento que no se comparte, se pierde

**Â¿QuÃ© compartimos?**

**ğŸ“š Conocimiento:**

- Documentation as Code (docs en el mismo repo)
- Runbooks automatizados
- Incident response procedures
- Lessons learned sessions

**ğŸ› ï¸ Herramientas y prÃ¡cticas:**

- Internal open source (cÃ³digo compartido entre equipos)
- Standardized tooling
- Common CI/CD templates
- Shared libraries

**ğŸ“Š Datos y mÃ©tricas:**

- Dashboards visibles para todos
- Performance metrics transparentes
- Error rates and trends
- Business impact metrics

**ğŸ’¡ Ejemplo de sharing en la prÃ¡ctica:**

```markdown
# Ejemplo: Runbook compartido

## Incident Response: API Down

### Detection

- Alert triggers when API response time > 5s
- Monitoring: Grafana dashboard shows red

### Investigation Steps

1. Check application logs: `kubectl logs -f deployment/api`
2. Check database connectivity: `pg_isready -h db-host`
3. Check resource usage: `kubectl top pods`

### Common Solutions

- If memory issue: Scale horizontally `kubectl scale deployment api --replicas=5`
- If database issue: Restart connection pool
- If code issue: Rollback to previous version

### Escalation

- If not resolved in 15min, escalate to @oncall-senior
- Update status page: status.company.com
```

## âš™ï¸ 3. CI/CD: La automatizaciÃ³n en acciÃ³n

### 3.1 Continuous Integration (CI) ğŸ”„

**DefiniciÃ³n:** PrÃ¡ctica de integrar cambios de cÃ³digo frecuentemente (varias veces al dÃ­a) con validaciÃ³n automÃ¡tica.

**ğŸ¯ Principios clave del CI:**

1. **Single source of truth:** Todo el cÃ³digo en un repositorio central
2. **Automate the build:** Un comando para compilar todo
3. **Test everything:** Cada commit ejecuta la suite de tests
4. **Fast feedback:** Builds rÃ¡pidos (< 10 minutos idealmente)
5. **Fix broken builds immediately:** Build roto = prioridad mÃ¡xima

**ğŸ”§ Componentes de un pipeline CI tÃ­pico:**

```mermaid
graph LR
    A[ğŸ‘¨â€ğŸ’» Developer<br/>Push] --> B[âš¡ Trigger<br/>Build]
    B --> C[ğŸ“¦ Compile<br/>Code]
    C --> D[ğŸ§ª Run<br/>Tests]
    D --> E[ğŸ“Š Quality<br/>Checks]
    E --> F[ğŸ”’ Security<br/>Scan]
    F --> G[ğŸ“‹ Generate<br/>Reports]
    G --> H{âœ… All Pass?}
    H -->|Yes| I[âœ… Build<br/>Success]
    H -->|No| J[âŒ Build<br/>Failure]
```

### 3.2 Continuous Deployment (CD) ğŸš€

**DefiniciÃ³n:** ExtensiÃ³n de CI donde cada cambio que pasa las pruebas se despliega automÃ¡ticamente a producciÃ³n.

**ğŸ¯ Niveles de CD:**

1. **Continuous Delivery:** CÃ³digo siempre listo para deploy, pero deploy es manual
2. **Continuous Deployment:** Deploy completamente automÃ¡tico a producciÃ³n

**ğŸ›¡ï¸ Estrategias de deployment seguro:**

| Estrategia        | DescripciÃ³n                                 | CuÃ¡ndo usar                     |
| ----------------- | ------------------------------------------- | ------------------------------- |
| **Blue-Green**    | Dos ambientes idÃ©nticos, switch instantÃ¡neo | Apps crÃ­ticas, rollback rÃ¡pido  |
| **Canary**        | Deploy gradual (5% â†’ 25% â†’ 100% usuarios)   | Apps con mucho trÃ¡fico          |
| **Rolling**       | Reemplazar instancias una por una           | Apps stateless, downtime mÃ­nimo |
| **Feature Flags** | CÃ³digo desplegado pero funciÃ³n desactivada  | Testing en producciÃ³n           |

### 3.3 Pipeline completo: De cÃ³digo a producciÃ³n

```mermaid
graph TD
    subgraph "ğŸ”§ Continuous Integration"
        A1[Code Commit] --> A2[Build]
        A2 --> A3[Unit Tests]
        A3 --> A4[Integration Tests]
        A4 --> A5[Code Quality]
        A5 --> A6[Security Scan]
    end

    subgraph "ğŸš€ Continuous Deployment"
        A6 --> B1[Deploy to Staging]
        B1 --> B2[E2E Tests]
        B2 --> B3[Performance Tests]
        B3 --> B4[Deploy to Production]
    end

    subgraph "ğŸ“Š Continuous Monitoring"
        B4 --> C1[Health Checks]
        C1 --> C2[Performance Metrics]
        C2 --> C3[User Analytics]
        C3 --> C4[Feedback Loop]
    end
```

## ğŸ’¡ Ejercicio PrÃ¡ctico (10 min)

**Escenario:** Tu empresa actualmente:

- Deploy manual cada viernes
- 3 equipos separados: Dev, QA, Ops
- Testing manual antes de cada release
- Bugs descubiertos por usuarios

**Tarea en grupos (3-4 personas):**

1. **Identifica 3 problemas** principales con este modelo
2. **PropÃ³n 2 cambios culturales** (pilar Culture de CAMS)
3. **DiseÃ±a 1 automatizaciÃ³n** que impactarÃ­a mÃ¡s (pilar Automation)
4. **Define 1 mÃ©trica** para medir el Ã©xito del cambio (pilar Measurement)

**Tiempo:** 7 minutos de discusiÃ³n + 3 minutos de presentaciÃ³n grupal

---

## ğŸ”š Resumen BLOQUE 3

**Conceptos clave cubiertos:**
âœ… DevOps como cultura, no herramienta
âœ… Modelo CAMS: Culture, Automation, Measurement, Sharing
âœ… CI/CD: IntegraciÃ³n y deployment continuo
âœ… Estrategias de deployment seguro
âœ… Pipeline completo DevOps

**Para el siguiente bloque:**
ğŸ¯ Testing automatizado en pipelines CI/CD

---

# ğŸ§ª BLOQUE 4: Testing en pipelines CI/CD (30 min)

_"En DevOps, el testing no es un gate que bloquea, es un safety net que permite ir mÃ¡s rÃ¡pido con confianza"_

## ğŸ“š Glosario de Testing en CI/CD

| TÃ©rmino                 | DefiniciÃ³n                                                                 | Ejemplo prÃ¡ctico                                                   |
| ----------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| **Test Pyramid**        | Estrategia de testing con mÃ¡s tests unitarios, menos E2E                   | 70% unit, 20% integration, 10% E2E                                 |
| **Smoke Tests**         | Pruebas bÃ¡sicas que verifican funcionalidad crÃ­tica despuÃ©s del deploy     | "Â¿La app arranca?", "Â¿La DB estÃ¡ conectada?"                       |
| **Regression Tests**    | Pruebas que verifican que nuevos cambios no rompan funcionalidad existente | Suite de tests que corre en cada deploy                            |
| **Contract Testing**    | Pruebas que verifican que APIs mantengan su contrato                       | "Este endpoint siempre devuelve un JSON con campo 'id'"            |
| **Chaos Engineering**   | Introducir fallos controlados para probar la resilencia                    | Apagar un servidor aleatoriamente para ver si el sistema sobrevive |
| **Shift-Right Testing** | Testing en producciÃ³n con usuarios reales                                  | A/B testing, feature flags, monitoring                             |

## ğŸ—ï¸ 1. Arquitectura de Testing en CI/CD

### 1.1 La Test Pyramid en el contexto del pipeline

```mermaid
graph TD
    subgraph "ğŸš€ CI/CD Pipeline"
        A[Code Commit]
        B[Build]
        C[Unit Tests<br/>âš¡ Fast, Many]
        D[Integration Tests<br/>ğŸ”— Medium, Some]
        E[Contract Tests<br/>ğŸ“‹ API Contracts]
        F[System Tests<br/>ğŸ§ª Slow, Few]
        G[Deploy to Staging]
        H[E2E Tests<br/>ğŸ­ Critical User Journeys]
        I[Performance Tests<br/>â±ï¸ Load & Stress]
        J[Security Tests<br/>ğŸ”’ Vulnerability Scans]
        K[Deploy to Production]
        L[Smoke Tests<br/>ğŸ’¨ Basic Health Checks]
        M[Monitoring<br/>ğŸ“Š Real User Data]
    end

    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> K
    K --> L
    L --> M
```

### 1.2 Testing en diferentes etapas del pipeline

| Etapa Pipeline  | Tipos de Tests        | Objetivo                 | Tiempo tÃ­pico | Si falla               |
| --------------- | --------------------- | ------------------------ | ------------- | ---------------------- |
| **Pre-commit**  | Linting, Format       | Calidad bÃ¡sica de cÃ³digo | < 30 segundos | Bloquea commit         |
| **Build**       | Unit Tests            | LÃ³gica individual        | 1-5 minutos   | Falla el build         |
| **Integration** | API Tests, DB Tests   | Componentes juntos       | 5-15 minutos  | No avanza a staging    |
| **Staging**     | E2E, Performance      | Sistema completo         | 15-60 minutos | No avanza a producciÃ³n |
| **Production**  | Smoke, Health Checks  | VerificaciÃ³n bÃ¡sica      | 1-2 minutos   | Rollback automÃ¡tico    |
| **Post-deploy** | Monitoring, A/B Tests | Experiencia real         | Continuo      | Alertas y mÃ©tricas     |

## âš¡ 2. Fast Feedback: Tests que ejecutan rÃ¡pido

### 2.1 OptimizaciÃ³n de Unit Tests

**ğŸ¯ Principios para tests rÃ¡pidos:**

```javascript
// âœ… BIEN: Test unitario rÃ¡pido, aislado
describe("CalculadoraDescuentos", () => {
  test("deberÃ­a aplicar descuento del 10% correctamente", () => {
    const calculadora = new CalculadoraDescuentos();
    const precio = 100;
    const descuento = 10;

    const resultado = calculadora.aplicarDescuento(precio, descuento);

    expect(resultado).toBe(90);
  });
});

// âŒ MAL: Test que depende de base de datos (lento)
describe("CalculadoraDescuentos", () => {
  test("deberÃ­a aplicar descuento desde DB", async () => {
    const db = await conectarBaseDatos();
    const calculadora = new CalculadoraDescuentos(db);

    const resultado = await calculadora.aplicarDescuentoDesdeDB(100, 1);

    expect(resultado).toBe(90);
  });
});
```

**ğŸš€ Estrategias para acelerar tests:**

1. **Mocking de dependencias externas** (DB, APIs, file system)
2. **ParalelizaciÃ³n** de test suites
3. **Test data builders** para setup rÃ¡pido
4. **In-memory databases** para tests de integraciÃ³n
5. **Smart test selection** (solo tests afectados por cambios)

### 2.2 Pre-commit hooks: El primer filtro

```javascript
// .husky/pre-commit
#!/usr/bin/env sh
. "$(dirname -- "$0")/_/husky.sh"

echo "ğŸ” Running pre-commit checks..."

# Formato de cÃ³digo
npm run format:check
if [ $? -ne 0 ]; then
  echo "âŒ Code formatting failed"
  exit 1
fi

# Linting
npm run lint
if [ $? -ne 0 ]; then
  echo "âŒ Linting failed"
  exit 1
fi

# Tests unitarios rÃ¡pidos
npm run test:unit
if [ $? -ne 0 ]; then
  echo "âŒ Unit tests failed"
  exit 1
fi

echo "âœ… Pre-commit checks passed!"
```

## ğŸ”— 3. Integration Testing en el pipeline

### 3.1 Contract Testing: Asegurando compatibilidad

**Â¿QuÃ© es Contract Testing?**
Verifica que los servicios mantengan sus "contratos" (APIs) sin necesidad de levantar todos los servicios.

```javascript
// Ejemplo con Pact.js
const { Pact } = require("@pact-foundation/pact");

describe("User Service Contract", () => {
  const provider = new Pact({
    consumer: "Frontend App",
    provider: "User Service",
  });

  test("should get user by ID", async () => {
    await provider
      .given("user exists")
      .uponReceiving("a request for user")
      .withRequest({
        method: "GET",
        path: "/users/123",
        headers: { Accept: "application/json" },
      })
      .willRespondWith({
        status: 200,
        headers: { "Content-Type": "application/json" },
        body: {
          id: 123,
          name: "John Doe",
          email: "john@example.com",
        },
      });

    const response = await userService.getUser(123);
    expect(response.id).toBe(123);
  });
});
```

### 3.2 Database Testing: Datos consistentes

```yaml
# docker-compose.test.yml
services:
  test-db:
    image: postgres:13
    environment:
      POSTGRES_DB: test_db
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
    ports:
      - "5433:5432"

  app-tests:
    build: .
    depends_on:
      - test-db
    environment:
      DATABASE_URL: postgresql://test:test@test-db:5432/test_db
    command: npm run test:integration
```

## ğŸ­ 4. End-to-End Testing: La experiencia completa del usuario

### 4.1 E2E Tests estratÃ©gicos, no exhaustivos

**ğŸ¯ Principio clave:** E2E debe cubrir **happy paths crÃ­ticos**, no todos los casos edge.

```javascript
// Ejemplo con Playwright
const { test, expect } = require("@playwright/test");

test("Usuario puede completar compra exitosamente", async ({ page }) => {
  // 1. Login
  await page.goto("/login");
  await page.fill("[data-testid=email]", "usuario@test.com");
  await page.fill("[data-testid=password]", "password123");
  await page.click("[data-testid=login-button]");

  // 2. Agregar producto al carrito
  await page.goto("/productos");
  await page.click("[data-testid=producto-123]");
  await page.click("[data-testid=agregar-carrito]");

  // 3. Checkout
  await page.goto("/carrito");
  await page.click("[data-testid=checkout]");

  // 4. Verificar compra exitosa
  await expect(page.locator("[data-testid=compra-exitosa]")).toBeVisible();
  await expect(page.locator("[data-testid=numero-orden]")).toContainText(
    /ORD-\d+/
  );
});
```

### 4.2 Visual Testing: UI consistente

```javascript
// Ejemplo con Percy o Chromatic
test("PÃ¡gina de checkout se ve correctamente", async ({ page }) => {
  await page.goto("/checkout");

  // Llenar formulario con datos de prueba
  await llenarFormularioCheckout(page);

  // Captura visual regression testing
  await percySnapshot(page, "Checkout Page - Filled Form");
});
```

## âš¡ 5. Performance Testing en el pipeline

### 5.1 Load Testing automatizado

```javascript
// k6 script para load testing
import http from "k6/http";
import { check, sleep } from "k6";

export let options = {
  stages: [
    { duration: "30s", target: 20 }, // Subir a 20 usuarios
    { duration: "1m", target: 20 }, // Mantener 20 usuarios
    { duration: "20s", target: 0 }, // Bajar a 0 usuarios
  ],
  thresholds: {
    http_req_duration: ["p(95)<200"], // 95% de requests < 200ms
    http_req_failed: ["rate<0.1"], // Menos del 10% de errores
  },
};

export default function () {
  let response = http.get("https://api.ejemplo.com/users");

  check(response, {
    "Status es 200": (r) => r.status === 200,
    "Respuesta en menos de 200ms": (r) => r.timings.duration < 200,
  });

  sleep(1);
}
```

### 5.2 Pipeline con performance gates

```yaml
# GitHub Actions ejemplo
name: CI/CD with Performance Testing

on: [push]

jobs:
  performance-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Deploy to staging
        run: ./deploy-staging.sh

      - name: Run load tests
        run: k6 run performance-test.js

      - name: Check performance thresholds
        run: |
          if [ $? -ne 0 ]; then
            echo "âŒ Performance tests failed - blocking deployment"
            exit 1
          fi
          echo "âœ… Performance tests passed"

      - name: Deploy to production
        if: success()
        run: ./deploy-production.sh
```

## ğŸ”’ 6. Security Testing integrado

### 6.1 SAST y DAST en el pipeline

```yaml
security-scan:
  runs-on: ubuntu-latest
  steps:
    - uses: actions/checkout@v2

    # Static Application Security Testing (SAST)
    - name: Run SAST scan
      uses: github/super-linter@v4
      env:
        DEFAULT_BRANCH: main
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    # Dependency vulnerability scan
    - name: Run vulnerability scan
      run: npm audit --audit-level high

    # Dynamic Application Security Testing (DAST)
    - name: Deploy to test environment
      run: ./deploy-test.sh

    - name: Run DAST scan
      run: |
        docker run --rm \
          -v $(pwd):/zap/wrk/:rw \
          owasp/zap2docker-stable:latest \
          zap-baseline.py -t https://test.myapp.com
```

## ğŸ’¨ 7. Smoke Tests: ValidaciÃ³n post-deployment

### 7.1 Health checks esenciales

```javascript
// smoke-tests.js
const axios = require("axios");

const PRODUCTION_URL = "https://api.miapp.com";

describe("Smoke Tests - Production Health", () => {
  test("API estÃ¡ respondiendo", async () => {
    const response = await axios.get(`${PRODUCTION_URL}/health`);
    expect(response.status).toBe(200);
    expect(response.data.status).toBe("healthy");
  });

  test("Base de datos estÃ¡ conectada", async () => {
    const response = await axios.get(`${PRODUCTION_URL}/health/database`);
    expect(response.status).toBe(200);
    expect(response.data.database).toBe("connected");
  });

  test("Servicios externos estÃ¡n disponibles", async () => {
    const response = await axios.get(`${PRODUCTION_URL}/health/external`);
    expect(response.status).toBe(200);
    expect(response.data.payment_gateway).toBe("available");
    expect(response.data.email_service).toBe("available");
  });

  test("Funcionalidad crÃ­tica funciona", async () => {
    // Test del happy path mÃ¡s importante
    const loginResponse = await axios.post(`${PRODUCTION_URL}/auth/login`, {
      email: "smoke-test@example.com",
      password: "test-password",
    });

    expect(loginResponse.status).toBe(200);
    expect(loginResponse.data.token).toBeDefined();
  });
});
```

### 7.2 Automated rollback basado en smoke tests

```yaml
deploy-with-rollback:
  runs-on: ubuntu-latest
  steps:
    - name: Deploy new version
      id: deploy
      run: |
        ./deploy.sh
        echo "DEPLOYMENT_ID=$(cat deployment.id)" >> $GITHUB_OUTPUT

    - name: Wait for deployment to be ready
      run: sleep 60

    - name: Run smoke tests
      id: smoke-tests
      run: npm run test:smoke
      continue-on-error: true

    - name: Rollback if smoke tests fail
      if: steps.smoke-tests.outcome == 'failure'
      run: |
        echo "âŒ Smoke tests failed - initiating rollback"
        ./rollback.sh ${{ steps.deploy.outputs.DEPLOYMENT_ID }}
        exit 1

    - name: Success notification
      if: steps.smoke-tests.outcome == 'success'
      run: echo "âœ… Deployment successful - all smoke tests passed"
```

## ğŸ’¡ Ejercicio PrÃ¡ctico Final (15 min)

**Escenario:** DiseÃ±a un pipeline de CI/CD para una aplicaciÃ³n web de e-commerce

**Requerimientos:**

- Frontend React + Backend Node.js + Base de datos PostgreSQL
- Deploy frecuente (varias veces por dÃ­a)
- Zero downtime deployments
- SLA 99.9% uptime

**Tarea en grupos (3-4 personas):**

1. **DiseÃ±a el pipeline** (7 min):

   - Â¿QuÃ© tests incluirÃ­as en cada etapa?
   - Â¿En quÃ© puntos agregarÃ­as quality gates?
   - Â¿QuÃ© estrategia de deployment usarÃ­as?

2. **Define mÃ©tricas de Ã©xito** (3 min):

   - Â¿CÃ³mo medirÃ­as que el pipeline funciona bien?
   - Â¿QuÃ© alertas configurarÃ­as?

3. **Presenta tu soluciÃ³n** (5 min):
   - Un grupo presenta su pipeline
   - DiscusiÃ³n grupal de ventajas/desventajas

---

## ğŸ”š Resumen BLOQUE 4

**Conceptos clave cubiertos:**
âœ… Test pyramid en contexto CI/CD
âœ… Fast feedback con pre-commit hooks
âœ… Contract testing para microservicios
âœ… E2E testing estratÃ©gico
âœ… Performance y security testing automatizado
âœ… Smoke tests y rollback automÃ¡tico

---

## ğŸ“ RESUMEN GENERAL DE LA CLASE

### ğŸ”„ Recorrido completo:

**BLOQUE 1:** Integramos testing en todo el SDLC con Shift-Left Testing
**BLOQUE 2:** Medimos calidad con mÃ©tricas DORA y Quality Gates  
**BLOQUE 3:** Adoptamos cultura DevOps con modelo CAMS
**BLOQUE 4:** Automatizamos testing en pipelines CI/CD

### ğŸ¯ Objetivos logrados:

âœ… **Integrar estrategias de testing** en cada fase del SDLC
âœ… **Implementar Shift-Left Testing** para calidad proactiva
âœ… **Comprender mÃ©tricas de calidad** y quality gates en DevOps
âœ… **Aplicar fundamentos DevOps y CI/CD** en testing automatizado

### ğŸš€ Para la prÃ³xima clase:

- **ImplementaciÃ³n prÃ¡ctica** de pipelines CI/CD
- **Hands-on** con herramientas reales (GitHub Actions, Jest, Playwright)
- **Workshop** de configuraciÃ³n de Quality Gates

### ğŸ“š Recursos para profundizar:

- **Libros:** "Accelerate" (DORA research), "The DevOps Handbook"
- **Herramientas:** GitHub Actions, GitLab CI, Jenkins, SonarQube
- **PrÃ¡ctica:** Crear tu propio pipeline CI/CD con un proyecto personal

---

**Â¡Gracias por participar! ğŸ™Œ**

_"El testing en la era DevOps no es sobre encontrar bugs, es sobre entregar valor con confianza y velocidad"_
